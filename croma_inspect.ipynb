{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import requests\n",
    "import json\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SpacyTextSplitter\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "\n",
    "def _clean_text(document_content):\n",
    "    return \" \".join(document_content.lower().strip().split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89023f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document():\n",
    "    valid_pages: List[Document] = [] # Explicitly type hint for clarity\n",
    "\n",
    "    column_names = [\n",
    "        \"pk_subscription\",\n",
    "        \"fck_customer_account\",\n",
    "        \"fck_subscription_plan\",\n",
    "        \"ek_subscription_state\",\n",
    "        \"ak_subscription_id\",\n",
    "        \"ck_start_date\",\n",
    "        \"end_date\",\n",
    "        \"billing_interval\",\n",
    "        \"subscription_price\",\n",
    "        \"subscription_feature_price\",\n",
    "        \"subscription_tracking_id\",\n",
    "        \"cancellation_date\",\n",
    "        \"cancellation_to_date\",\n",
    "        \"created_by\",\n",
    "        \"created_on\",\n",
    "        \"updated_by\",\n",
    "        \"updated_on\",\n",
    "    ]\n",
    "\n",
    "    # Iterate through each column name and create a separate Document for it\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        valid_pages.append(\n",
    "            Document(\n",
    "                page_content=col_name,\n",
    "                metadata={\"column_index\": i, \"column_name\": col_name, \"source\": \"schema_definition\"}\n",
    "            )\n",
    "        )\n",
    "    return valid_pages\n",
    "\n",
    "# Example of how you would then use it\n",
    "documents = load_document()\n",
    "print(\"Documents loaded:\", documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=token,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"embeddings: \",embeddings)\n",
    "documents = load_document()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not documents:\n",
    "    print(\"No valid documents found to process. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "    # Step 1: Split documents into smaller chunks (important for RAG and efficient retrieval) | # This helps in handling large documents and ensures that each chunk\n",
    "    # is semantically coherent for better retrieval.\n",
    "    \n",
    "    text_splitter = SpacyTextSplitter(\n",
    "        chunk_size=10,      # The maximum size of each chunk\n",
    "        chunk_overlap=0,    # The overlap between chunks to maintain context\n",
    "        # length_function=len,\n",
    "        # is_separator_regex=False,\n",
    "    )\n",
    "    # The split_documents method takes a list of Document objects and returns\n",
    "    # a new list of smaller Document objects.\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    print(f\"Split documents into {len(chunked_documents)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing in-memory ChromaDB. Data will NOT be persisted to disk.\")\n",
    "print(\"This means the database will be cleared when the script finishes.\")\n",
    "\n",
    "# KEY CHANGE: Removed persist_directory\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embeddings,\n",
    "    # persist_directory is removed for in-memory\n",
    ")\n",
    "print(\"In-memory ChromaDB successfully initialized and documents are stored.\")\n",
    "print(\"You can now query your in-memory Chroma database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user's request in normal language.\n",
    "query = \"the cancel date is jun 15 and start date is jan 1 all are year 2025 make a json using this information\"\n",
    "print(f\"\\nPerforming similarity search for the query: '{query}'\")\n",
    "# Find the most relevant columns from our AI memory.\n",
    "docs = vectordb.similarity_search(query, k=4)\n",
    "print(\"Similarity search complete.\")\n",
    "\n",
    "if docs:\n",
    "    print(f\"\\nFound {len(docs)} relevant document chunks:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\n--- Relevant Document Chunk {i+1} ---\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "\n",
    "    # --- NEW CODE TO INTERACT WITH LOCAL LLM ---\n",
    "    print(\"\\n--- SENDING DATA TO LOCAL LLM ---\")\n",
    "\n",
    "    # 1. Gather the context from the retrieved documents\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # 2. Define the system prompt and the user query\n",
    "    system_prompt = \"Your output MUST be a JSON object. Do not include any other text or explanation in your response. Based on the following database columns, identify the most relevant ones for the user's query.\"\n",
    "    user_query = f\"User Query: '{query}'\\n\\nRelevant Columns:\\n{context}\"\n",
    "\n",
    "    # 3. Construct the full prompt for the model\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_query}\"\n",
    "\n",
    "    # 4. Format the JSON payload for the API request\n",
    "    api_url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\"model\": \"qwen3:0.6b\", \"prompt\": full_prompt, \"stream\": False}\n",
    "\n",
    "    print(\"\\nSending the following payload to the local model:\")\n",
    "    print(json.dumps(payload, indent=2))\n",
    "\n",
    "    try:\n",
    "        # 5. Send the POST request to the local model\n",
    "        response = requests.post(api_url, json=payload)\n",
    "        response.raise_for_status()  # This will raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        # 6. Process the response\n",
    "        response_json = response.json()\n",
    "\n",
    "        # The actual generated content is often in a 'response' or 'content' key.\n",
    "        model_output_str = response_json.get(\"response\", \"\")\n",
    "\n",
    "        print(\"\\n--- RESPONSE FROM LOCAL LLM ---\")\n",
    "        print(f\"Raw model output string: {model_output_str}\")\n",
    "\n",
    "        # --- NEW & IMPROVED JSON EXTRACTION LOGIC ---\n",
    "        if not model_output_str:\n",
    "            print(\"Model returned an empty response.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Find the first occurrence of '{' and the last occurrence of '}'\n",
    "                start_index = model_output_str.find(\"{\")\n",
    "                end_index = model_output_str.rfind(\"}\")\n",
    "\n",
    "                if start_index != -1 and end_index != -1 and end_index > start_index:\n",
    "                    # Extract the potential JSON substring\n",
    "                    json_substring = model_output_str[start_index : end_index + 1]\n",
    "\n",
    "                    # Now, try to parse this cleaned substring\n",
    "                    structured_output = json.loads(json_substring)\n",
    "\n",
    "                    print(\"\\nSuccessfully extracted and parsed JSON output:\")\n",
    "                    print(json.dumps(structured_output, indent=2))\n",
    "                else:\n",
    "                    print(\n",
    "                        \"\\nError: Could not find a valid JSON object within the model's output.\"\n",
    "                    )\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(\n",
    "                    \"\\nError: The model's output contained a string that looked like JSON, but was invalid.\"\n",
    "                )\n",
    "                print(f\"Attempted to parse: {json_substring}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during JSON parsing: {e}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No relevant documents found to pass to the local model.\")\n",
    "\n",
    "print(\"\\n--- LOCAL LLM INTERACTION COMPLETE ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
